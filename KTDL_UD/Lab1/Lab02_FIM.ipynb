{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "DSYvP0n8RjYH"
   },
   "source": [
    "# Lab02: Frequent itemset mining\n",
    "\n",
    "- Student ID: 20120113\n",
    "- Student name: Lê Nguyên Khang\n",
    "\n",
    "**How to do your homework**\n",
    "\n",
    "\n",
    "You will work directly on this notebook; the word `TODO` indicate the parts you need to do.\n",
    "\n",
    "You can discuss ideas with classmates as well as finding information from the internet, book, etc...; but *this homework must be your*.\n",
    "\n",
    "**How to submit your homework**\n",
    "\n",
    "Before submitting, rerun the notebook (`Kernel` ->` Restart & Run All`).\n",
    "\n",
    "Then create a folder named `ID` (for example, if your ID is 1234567, then name the folder `1234567`) Copy file notebook to this folder, compress and submit it on moodle.\n",
    "\n",
    "**Contents:**\n",
    "\n",
    "- Frequent itemset mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXZ5gCVaRjYa"
   },
   "source": [
    "# 1. Preliminaries\n",
    "## This is how it all started ...\n",
    "- Rakesh Agrawal, Tomasz Imielinski, Arun N. Swami: Mining Association Rules between Sets of Items in Large Databases. SIGMOD Conference 1993: 207-216\n",
    "- Rakesh Agrawal, Ramakrishnan Srikant: Fast Algorithms for Mining Association Rules in Large Databases. VLDB 1994: 487-499\n",
    "\n",
    "**These two papers are credited with the birth of Data Mining**\n",
    "## Frequent itemset mining (FIM)\n",
    "\n",
    "Find combinations of items (itemsets) that occur frequently.\n",
    "## Applications\n",
    "- Items = products, transactions = sets of products someone bought in one trip to the store.\n",
    "$\\Rightarrow$ items people frequently buy together.\n",
    "    + Example: if people usually buy bread and coffee together, we run a sale of bread to attract people attention and raise price of coffee.\n",
    "- Items = webpages, transactions = words. Unusual words appearing together in a large number of documents, e.g., “Brad” and “Angelina,” may indicate an interesting relationship.\n",
    "- Transactions = Sentences, Items = Documents containing those sentences. Items that appear together too often could represent plagiarism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8vAJ8A2RjYi"
   },
   "source": [
    "## Transactional Database\n",
    "A transactional database $D$ consists of $N$ transactions: $D=\\left\\{T_1,T_2,...,T_N\\right\\}$. A transaction $T_n \\in D (1 \\le n \\le N)$ contains one or more items and that $I= \\left\\{ i_1,i_2,…,i_M \\right\\}$ is the set of distinct items in $D$, $T_n \\subset I$. Commonly, a transactional database is represented by a flat file instead of a database system: items are non-negative integers, each row represents a transaction, items in a transaction separated by space.\n",
    "\n",
    "Example: \n",
    "\n",
    "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 \n",
    "\n",
    "30 31 32 \n",
    "\n",
    "33 34 35 \n",
    "\n",
    "36 37 38 39 40 41 42 43 44 45 46 \n",
    "\n",
    "38 39 47 48 \n",
    "\n",
    "38 39 48 49 50 51 52 53 54 55 56 57 58 \n",
    "\n",
    "32 41 59 60 61 62 \n",
    "\n",
    "3 39 48 \n",
    "\n",
    "63 64 65 66 67 68 \n",
    "\n",
    "\n",
    "\n",
    "# Definition\n",
    "\n",
    "- Itemset: A collection of one or more items.\n",
    "    + Example: {1 4 5}\n",
    "- **k-itemset**: An itemset that contains k items.\n",
    "- Support: Frequency of occurrence of an itemset.\n",
    "    + Example: From the example above, item 3 appear in 2 transactions so its support is 2.\n",
    "- Frequent itemset: An itemset whose support is greater than or equal to a `minsup` threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdykKxr6RjY-"
   },
   "source": [
    "# The Apriori Principle\n",
    "- If an itemset is frequent, then all of its subsets must also be frequent.\n",
    "- If an itemset is not frequent, then all of its supersets cannot be frequent.\n",
    "- The support of an itemset never exceeds the support of its subsets.\n",
    "$$ \\forall{X,Y}: (X \\subseteq Y) \\Rightarrow s(X)\\ge s(Y)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvfMR7-CRjZB"
   },
   "source": [
    "# 2. Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9gZh4DORjZD"
   },
   "source": [
    "## The Apriori algorithm\n",
    "Suppose:\n",
    "\n",
    "$C_k$ candidate itemsets of size k.\n",
    "\n",
    "$L_k$ frequent itemsets of size k.\n",
    "\n",
    "The level-wise approach of Apriori algorithm can be descibed as follow:\n",
    "1. k=1, $C_k$ = all items.\n",
    "2. While $C_k$ not empty:\n",
    "    3. Scan the database to find which itemsets in $C_k$ are frequent and put them into $L_k$.\n",
    "    4. Use $L_k$ to generate a collection of candidate itemsets $C_{k+1}$ of size k+1.\n",
    "    5. k=k+1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qF9xHOBLRjZJ"
   },
   "source": [
    "### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "7F0lUOSuRjZN"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OogwdcLRjZf"
   },
   "source": [
    "### Read data\n",
    "First we have to read data from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "id": "U2bsGrTERjZg"
   },
   "outputs": [],
   "source": [
    "\n",
    "def readData(path):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------\n",
    "        path: path of database D.\n",
    "         \n",
    "    --------------------------\n",
    "    Returns\n",
    "        data: a dictionary for representing database D\n",
    "                 - keys: transaction tids\n",
    "                 - values: itemsets.\n",
    "        s: support of distict items in D.\n",
    "    \"\"\"\n",
    "    data={}\n",
    "    s=defaultdict(lambda: 0) # Initialize a dictionary for storing support of items in I.  \n",
    "    with open(path,'rt') as f:\n",
    "        tid=1;\n",
    "        for line in f:\n",
    "            itemset=set(map(int,line.split())) # a python set is a native way for storing an itemset.\n",
    "            for item in itemset:  \n",
    "                s[item]+=1     #Why don't we compute support of items while reading data?\n",
    "            data[tid]= itemset\n",
    "            tid+=1\n",
    "    \n",
    "    return data, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSTC78WURjZu"
   },
   "source": [
    "### Tree Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGAkmuXtRjZw"
   },
   "source": [
    "**I gave you pseudo code of Apriori algorithm above but we implement Tree Projection. Tell me the differences of two algorithms.**\n",
    "\n",
    "\n",
    "**TODO:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joinset(a, b):\n",
    "    '''\n",
    "    Parameters\n",
    "    -------------------\n",
    "        2 itemsets a and b (of course they are at same branch in search space)\n",
    "\n",
    "    -------------------\n",
    "    return\n",
    "        ret: itemset generated by joining a and b\n",
    "    '''\n",
    "    ret = set(a + b)\n",
    "    ret = list(sorted(ret))\n",
    "    return ret\n",
    "\n",
    "class TP:\n",
    "    def __init__(self, data=None, s=None, minSup=None):\n",
    "        self.data = data\n",
    "        self.s = {}\n",
    "\n",
    "        for key, support in sorted(s.items(), key=lambda item: item[1]):\n",
    "            self.s[key] = support\n",
    "        # TODO: why should we do this, answer it at the markdown below?\n",
    "\n",
    "        self.minSup = minSup\n",
    "        self.L = {}  # Store frequent itemsets mined from database\n",
    "        self.runAlgorithm()\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"\n",
    "        Initialize search space at first step\n",
    "        --------------------------------------\n",
    "        We represent our search space in a tree structure\n",
    "        \"\"\"\n",
    "        tree = {}\n",
    "\n",
    "        search_space = {}\n",
    "        for item, support in self.s.items():\n",
    "            search_space[item] = {}\n",
    "\n",
    "            search_space[item]['itemset'] = [item]\n",
    "            ''' \n",
    "            python set does not remain elements order\n",
    "            so we use a list to extend it easily when create new itemset \n",
    "            but why we store itemset in data by a python set???? '''\n",
    "            # TODO: study about python set and its advantages,\n",
    "            # answer at the markdown below.\n",
    "\n",
    "            search_space[item]['pruned'] = False\n",
    "            # TODO:\n",
    "            # After finish implementing the algorithm tell me why should you use this\n",
    "            # instead of delete item directly from search_space and tree.\n",
    "\n",
    "            search_space[item]['support'] = support\n",
    "\n",
    "            tree[item] = {}\n",
    "            '''\n",
    "            Why should i store an additional tree (here it called tree)? \n",
    "            Answer: This really help in next steps.\n",
    "\n",
    "            Remember that there is always a big gap from theory to practicality\n",
    "            and implementing this algorithm in python is not as simple as you think.\n",
    "            '''\n",
    "\n",
    "        return tree, search_space\n",
    "\n",
    "    def computeItemsetSupport(self, itemset):\n",
    "\n",
    "        '''Return support of itemset'''\n",
    "        # TODO (hint: this is why i use python set in data)\n",
    "        pat = frozenset(itemset)\n",
    "        support = 0 \n",
    "        for d in self.data:\n",
    "            ds = frozenset(self.data[d])\n",
    "            if pat.issubset(ds):\n",
    "                support += 1\n",
    "        return support\n",
    "\n",
    "    def get_sub_tree(self, k, tree, search_space, itter_node):\n",
    "        if k == 0:\n",
    "            return search_space[itter_node]['support']\n",
    "        subtree = search_space[itter_node]\n",
    "        for node in subtree.keys():\n",
    "            k-=1\n",
    "            self.get_sub_tree(k,tree,search_space,node)\n",
    "\n",
    "\n",
    "    def prune(self, k, tree, search_space):\n",
    "\n",
    "        '''\n",
    "        In this method we will find out which itemset in current search space is frequent\n",
    "        itemset then add it to L[k]. In addition, we prune those are not frequent itemsets.\n",
    "        '''\n",
    "        if self.L.get(k) is None: self.L[k] = []\n",
    "        # TODO\n",
    "        for itemset in search_space.keys():\n",
    "            if search_space[itemset]['pruned']: continue\n",
    "            if search_space[itemset]['support'] < self.minSup:\n",
    "                search_space[itemset]['pruned'] = True\n",
    "            else:\n",
    "                self.L[k].append(search_space[itemset]['itemset'])\n",
    "\n",
    "    def generateSearchSpace(self, k, tree, search_space):\n",
    "        '''\n",
    "        Generate search space for exploring k+1 itemset. (Recursive function)\n",
    "        '''\n",
    "        items = list(tree.keys())\n",
    "        ''' print search_space.keys() you will understand  \n",
    "         why we need an additional tree, '''\n",
    "        l = len(items)\n",
    "        self.prune(k, tree, search_space)\n",
    "        if l == 0: return  # Stop condition\n",
    "        for i in range(l - 1):\n",
    "            sub_search_space = {}\n",
    "            sub_tree = {}\n",
    "            a = items[i]\n",
    "            if search_space[a]['pruned']: continue\n",
    "\n",
    "            for j in range(i + 1, l):\n",
    "                b = items[j]\n",
    "                search_space[a][b] = {}\n",
    "                tree[a][b] = {}\n",
    "                # You really need to understand what am i doing here before doing work below.\n",
    "                # (Hint: draw tree and search space to draft).\n",
    "\n",
    "                # TODO:\n",
    "                # First create newset using join set\n",
    "                newset = tuple(joinset(search_space[a]['itemset'], search_space[b]['itemset']))\n",
    "\n",
    "                # Second add newset to search_space\n",
    "                sub_search_space[newset] = {}\n",
    "                sub_search_space[newset]['itemset'] = newset\n",
    "                sub_search_space[newset]['support'] = self.computeItemsetSupport(newset)\n",
    "                sub_search_space[newset]['pruned'] = False\n",
    "                sub_tree[newset] = {}\n",
    "                \n",
    "            #  Generate sub_search_space for k+1-itemset\n",
    "            self.generateSearchSpace(k + 1, sub_tree, sub_search_space)\n",
    "\n",
    "    def runAlgorithm(self):\n",
    "        tree, search_space = self.initialize()  # generate search space for 1-itemset\n",
    "        self.generateSearchSpace(1, tree, search_space)\n",
    "\n",
    "    def miningResults(self):\n",
    "        return self.L\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tMTpwxLRjZ-"
   },
   "source": [
    "Ok, let's test on a typical dataset `chess`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "gLygYqiYRjZ-"
   },
   "outputs": [],
   "source": [
    "data, s= readData('chess.txt')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [[48], [56], [66], [34], [62], [7], [36], [60], [40], [29], [52], [58]], 2: [(48, 52), (48, 58), (29, 56), (52, 56), (56, 58), (60, 66), (29, 66), (52, 66), (58, 66), (34, 40), (29, 34), (34, 52), (34, 58), (60, 62), (40, 62), (29, 62), (52, 62), (58, 62), (7, 60), (7, 40), (7, 29), (7, 52), (7, 58), (36, 60), (36, 40), (29, 36), (36, 52), (36, 58), (40, 60), (29, 60), (52, 60), (58, 60), (29, 40), (40, 52), (40, 58), (29, 52), (29, 58), (52, 58)], 3: [(48, 52, 58), (29, 52, 56), (29, 56, 58), (52, 56, 58), (29, 60, 66), (52, 60, 66), (58, 60, 66), (29, 52, 66), (29, 58, 66), (52, 58, 66), (29, 34, 40), (34, 40, 52), (34, 40, 58), (29, 34, 52), (29, 34, 58), (34, 52, 58), (29, 60, 62), (52, 60, 62), (58, 60, 62), (29, 40, 62), (40, 52, 62), (40, 58, 62), (29, 52, 62), (29, 58, 62), (52, 58, 62), (7, 40, 60), (7, 29, 60), (7, 52, 60), (7, 58, 60), (7, 29, 40), (7, 40, 52), (7, 40, 58), (7, 29, 52), (7, 29, 58), (7, 52, 58), (36, 40, 60), (29, 36, 60), (36, 52, 60), (36, 58, 60), (29, 36, 40), (36, 40, 52), (36, 40, 58), (29, 36, 52), (29, 36, 58), (36, 52, 58), (29, 40, 60), (40, 52, 60), (40, 58, 60), (29, 52, 60), (29, 58, 60), (52, 58, 60), (29, 40, 52), (29, 40, 58), (40, 52, 58), (29, 52, 58)], 4: [(29, 52, 56, 58), (29, 52, 60, 66), (29, 58, 60, 66), (52, 58, 60, 66), (29, 52, 58, 66), (29, 34, 40, 52), (29, 34, 40, 58), (34, 40, 52, 58), (29, 34, 52, 58), (29, 58, 60, 62), (52, 58, 60, 62), (29, 40, 52, 62), (29, 40, 58, 62), (40, 52, 58, 62), (29, 52, 58, 62), (7, 40, 58, 60), (7, 29, 52, 60), (7, 29, 58, 60), (7, 52, 58, 60), (7, 29, 40, 52), (7, 29, 40, 58), (7, 40, 52, 58), (7, 29, 52, 58), (29, 36, 40, 60), (36, 40, 52, 60), (36, 40, 58, 60), (29, 36, 52, 60), (29, 36, 58, 60), (36, 52, 58, 60), (29, 36, 40, 52), (29, 36, 40, 58), (36, 40, 52, 58), (29, 36, 52, 58), (29, 40, 52, 60), (29, 40, 58, 60), (40, 52, 58, 60), (29, 52, 58, 60), (29, 40, 52, 58)], 5: [(29, 52, 58, 60, 66), (29, 34, 40, 52, 58), (29, 40, 52, 58, 62), (7, 29, 52, 58, 60), (7, 29, 40, 52, 58), (29, 36, 40, 52, 60), (29, 36, 40, 58, 60), (36, 40, 52, 58, 60), (29, 36, 52, 58, 60), (29, 36, 40, 52, 58), (29, 40, 52, 58, 60)], 6: [(29, 36, 40, 52, 58, 60)]}\n"
     ]
    }
   ],
   "source": [
    "a=TP(data=data,s=s, minSup=3000)\n",
    "print(a.miningResults())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Mp0RFbw-RjaU"
   },
   "source": [
    "### Answer questions here:\n",
    "**Why don't we compute support of items while reading data?**\n",
    "- Đầu tiên chúng ta tính toán độ hổ trợ của các items trong khi đọc dữ liệu để từ đó biết được items nào có khả năng tạo tập phổ biết lớn hơn. Sau đó khi mỗi itemset được tạo thành ta mới tính support của nó vì các itemset mới được tạo dựa vào tập ứng viên vượt qua ngưỡng support trước đó, việc này làm giảm các itemset được tạo thành để tính support và làm cho thuật toán hiệu quả hơn. Bằng cách này giúp thuật toán thực hiện được theo cách cân bằng giữa độ hiệu quả và độ chính xác.\n",
    "\n",
    "**why should we do sort**\n",
    "- Việc sắp xếp các mục dựa trên tần suất xuất hiện của chúng trong các giao dịch của tập dữ liệu là một bước quan trọng để cải thiện hiệu quả của thuật toán. Khi sắp xếp cho phép thuật toán tập trung đầu tiên vào các mục thường xuyên nhất, có khả năng xảy ra cùng nhau trong nhiều giao dịch, trước khi xem xét các mục ít thường xuyên hơn. Ngoài ra, khi sắp xếp thuật toán có thể thực hiện cắt tỉa, đây là quá trình loại bỏ các tập mục ứng viên không thể phổ biến dựa trên các giá trị hỗ trợ của các tập con của chúng làm giảm không gian tìm kiếm và tăng tốc thuật toán.\n",
    "\n",
    "**study about python set and its advantages ?**\n",
    "- Loại bỏ các mục trùng lặp: Khi có các itemset trùng lặp, tập set sẽ cho ta biết key của itemset ấy đã tồn tại hay chưa.\n",
    "- Truy xuất nhanh: Khi cần gọi kiểm tra itemset có support bao nhiêu? đã được prune hay chưa? được lưu trước đó thì dùng tập set làm việc này rất nhanh.\n",
    "- Hiệu quả về bộ nhớ: Vì tập set có thể tiết kiệm bộ nhớ hơn so với danh sách khi xử lý các tập dữ liệu lớn.\n",
    "- Tổ chức lưu trữ itemset theo tree, search_space hiệu quả linh hoạt có thể mở rộng tùy ý dễ dàng đồng thời có thể kết hợp với kiểu list trong quá trình lưu trữ. \n",
    "- Các phép toán giao và hợp: Các tập set trong Python cũng có các phép toán giao và hợp hiệu quả, được sử dụng trong thuật toán để tạo ra các tập ứng viên có kích thước tăng dần.\n",
    "\n",
    "-> Thuật toán có sự cải thiện đáng kể về hiệu quả và hiệu xuất khi sử dụng tập set.\n",
    "\n",
    "**After finish implementing the algorithm tell me why should you use this? Instead of delete item directly from search_space and tree.**\n",
    "- Khả năng mất thông tin: Khi bạn xóa một mục khỏi không gian tìm kiếm và cây có thể sẽ mất thông tin về hỗ trợ của mục đó và mối quan hệ của mục đó với các mục khác trong tập dữ liệu. Điều này có thể khiến việc phân tích kết quả của thuật toán và xác định các mẫu thú vị trở nên khó khăn và có thể có sai sót bên trong.\n",
    "- Khả năng xảy ra lỗi: Việc xóa các mục trực tiếp khỏi không gian tìm kiếm và cây có thể dễ bị lỗi, đặc biệt với dữ liệu có kích thước lớn. Điều này có thể dẫn đến lỗi và kết quả không chính xác cho thuật toán.\n",
    "- Sau khi một mục bị xóa khỏi không gian và cây tìm kiếm, mục ấy có thể khó đảo ngược thay đổi và khôi phục trạng thái ban đầu. Đây có thể là vấn đề khi ta cần sửa đổi thuật toán hoặc phân tích kết quả theo những cách khác nhau.\n",
    "- Giảm tính linh hoạt: Bằng cách xóa trực tiếp các mục, có thể sẽ bị hạn chế tính linh hoạt của thuật toán để thích ứng với các bộ dữ liệu và các yêu cầu phân tích khác nhau. Đây có thể là vấn đề nếu khi ta tái sử dụng thuật toán cho các dự án hoặc bộ dữ liệu khác nhau.\n",
    "- Ngược lại, bằng cách triển khai thuật toán mà không xóa trực tiếp các mục khỏi không gian và cây tìm kiếm, ta có thể bảo toàn thông tin đầy đủ về tập dữ liệu và mối quan hệ của nó giữa các mục. Điều này có thể giúp phân tích kết quả và xác định các tập phổ biến trở nên dễ dàng hơn.\n",
    "\n",
    "**Apriori algorithm and Tree Projection, tell me the differences of two algorithms.**\n",
    "- Thuật toán Apriori sử dụng cách tiếp cận \"từ dưới lên\", trong đó thuật toán bắt đầu bằng cách xác định tất cả các mục riêng lẻ phổ biến (tập mục có kích thước 1) trong tập dữ liệu, sau đó lặp lại tạo các tập mục lớn hơn bằng cách nối các tập mục phổ biến từ lần lặp trước. Thuật toán cắt bớt các tập phổ biến ở mỗi lần lặp để giảm không gian tìm kiếm và cải thiện hiệu suất. Thuật toán phải quét toàn bộ tập dữ liệu nhiều lần cho mỗi lần lặp lại, điều này có thể tốn kém về mặt tính toán đối với các tập dữ liệu lớn.\n",
    "\n",
    "- Mặt khác, thuật toán Tree Projection là một cách tiếp cận \"từ trên xuống\" hoạt động bằng cách xây dựng một cây tiền tố từ tập dữ liệu. Cây tiền tố sau đó được chiếu để xác định các tập phổ biến bằng cách khám phá đệ quy các nút của cây. Trong mỗi lời gọi đệ quy, thuật toán sẽ kiểm tra xem nút hiện tại có thể được mở rộng thành tập phổ biến hay không và cắt bớt cây con nếu không. Thuật toán chỉ quét tập dữ liệu một lần để xây dựng cây tiền tố, đây có thể là cách tiếp cận hiệu quả hơn cho các tập dữ liệu lớn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnVm8wYIRjaV"
   },
   "source": [
    "# 3. Churn analysis\n",
    "\n",
    "In this section, you will use frequent itemset mining technique to analyze `churn` dataset (for any purposes). \n",
    "\n",
    "*Remember this dataset is not represented as a transactional database, first thing that you have to do is transforming it into a flat file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def readData_churn(path):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------\n",
    "        path: path of database D.\n",
    "         \n",
    "    --------------------------\n",
    "    Returns\n",
    "        data: a dictionary for representing database D\n",
    "                 - keys: transaction tids\n",
    "                 - values: itemsets.\n",
    "        s: support of distict items in D.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    s = defaultdict(lambda: 0)\n",
    "    with open(path,'rt') as f:\n",
    "        tid = 0\n",
    "        for line in f:\n",
    "            if tid == 0:\n",
    "                id = line.strip('\\n?').split(',')\n",
    "            else:\n",
    "                itemset = line.strip(\".\\n\").split(',')\n",
    "                row = []\n",
    "                i = 0\n",
    "                for item in itemset:  \n",
    "                    value = id[i] + ':' + item\n",
    "                    s[value]+=1\n",
    "                    row.append(value)\n",
    "                    i += 1\n",
    "                data[tid]= row\n",
    "\n",
    "            tid+=1\n",
    "    return data, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [['VMail Plan:no'], ['VMail Message:0'], ['Churn:False'], [\"Int'l Plan:no\"]], 2: [('VMail Message:0', 'VMail Plan:no'), ('Churn:False', 'VMail Plan:no'), (\"Int'l Plan:no\", 'VMail Plan:no'), ('Churn:False', 'VMail Message:0'), (\"Int'l Plan:no\", 'VMail Message:0'), ('Churn:False', \"Int'l Plan:no\")], 3: [('Churn:False', 'VMail Message:0', 'VMail Plan:no'), (\"Int'l Plan:no\", 'VMail Message:0', 'VMail Plan:no')], 4: []}\n"
     ]
    }
   ],
   "source": [
    "data2, s2 = readData_churn('churn.txt')\n",
    "a2 = TP(data=data2, s=s2, minSup=2000)\n",
    "print(a2.miningResults())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FzxGs7RRjaX"
   },
   "source": [
    "# 4 References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "doYH4biqR_N7"
   },
   "source": [
    "Feel free to send questions to my email address: nnduc@fit.hcmus.edu.vn\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Lab01 - Frequent itemset mining.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
